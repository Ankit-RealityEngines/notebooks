{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with Embeddings Artifacts for Plug and Play\n",
    "\n",
    "The plug and play use case provides an opportunity for customers to train their own models and leverage our infrastructure and abstractions to get their models hosted and running on a scalable service with an easy to integrate API. This notebook covers the creation of a model with embeddings, the artifacts that are required by our service, and the resulting API.\n",
    "\n",
    "Below are the imports that we'll be using to create/train the model, as well as generate the required artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Model with Embeddings - Word2Vec\n",
    "\n",
    "To demonstrate the artifacts required and to provide some recommendations on the overall structure of the model, we will create a model that generates predictions of embedding components. The result is computed as the minimum distance to elements of an embedding dataset with a configurable distance function. The resulting artifacts can be uploaded to the Abacus.AI platform where they can be hosted as a deployment. The artifacts produced are:\n",
    "* tensorflow saved model\n",
    "* embedding dataset\n",
    "* verification samples (optional)\n",
    "\n",
    "So first let's get our data using tensorflow_datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, test_data), dataset_info = tfds.load(\n",
    "    'imdb_reviews/subwords8k', \n",
    "    split = (tfds.Split.TRAIN, tfds.Split.TEST), \n",
    "    with_info=True, as_supervised=True)\n",
    "\n",
    "train_batches = train_data.shuffle(1000).padded_batch(10)\n",
    "test_batches = test_data.shuffle(1000).padded_batch(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model\n",
    "\n",
    "Here we have defined the model, taking special care to name the input. This is because when the model is hosted, our api accepts multiple types of inputs and it needs to be able to determine what gets passed on to the model. To resolve this, we inspect the model to discover its input tensor(s). Below, when defining the input, we have named it as tokens. As a result, the prediction api will - look for the “tokens” parameter, take its value, convert it into a tensor, and pass it on to the model.\n",
    "\n",
    "Let’s examine the following example curl request:\n",
    "```bash\n",
    "curl --globoff \"http://abacus.ai/api/v0/predict?deploymentToken=foobar&deploymentId=baz&notSent=deadbeef&tokens=[[123,456]]\"\n",
    "```\n",
    "Of all the query parameters, only `tokens=[[123,456]]` will be converted into a tensor to be passed into the model. The `deploymentToken` and `deploymentId` are required parameters for our API and the `notSent=deadbeef` will be dropped. If instead we wanted the query parameter to be `abacusIsAmazing`, we could name the `InputLayer` to be `abacusIsAmazing` and then the url will look like this (with the `notSent` removed):\n",
    "```bash\n",
    "curl --globoff \"http://abacus.ai/api/v0/predict?deploymentToken=foobar&deploymentId=baz&abacusIsAmazing=[[123,456]]\"\n",
    "```\n",
    "Let’s take another  example where we will stick with a more descriptive token name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = dataset_info.features['text'].encoder\n",
    "embedding_dim=16\n",
    "\n",
    "input_tokens = tf.keras.layers.Input(shape=(None,), name='tokens')\n",
    "embedding_layer = tf.keras.layers.Embedding(encoder.vocab_size, embedding_dim, name='embedding')\n",
    "embedding_output = embedding_layer(input_tokens)\n",
    "global_avg_output = tf.keras.layers.GlobalAveragePooling1D(name='avg_pooling')(embedding_output)\n",
    "relu_output = tf.keras.layers.Dense(16, activation='relu')(global_avg_output)\n",
    "dense_output = tf.keras.layers.Dense(1)(relu_output)\n",
    "model = tf.keras.Model(inputs=[input_tokens], outputs=[dense_output], name='word2vec')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we kick off the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_batches,\n",
    "    epochs=10,\n",
    "    validation_data=test_batches, validation_steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restructuring the Model for our use-case\n",
    "\n",
    "Now that we have a trained model, let’s make some model structure changes in preparation for use in Abacus.AI. We would like this model to output a vector, in this case of size 16, to match the embedding size, that can then be used with the embeddings we extract later in this notebook to get a list of synonymous words. To do so, we'll create a new model, but instead route the output from the existing GlobalAveragePooling1D layer into a Lambda layer to reshape the output into a vector of 16 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_avg_output = model.get_layer('avg_pooling').output\n",
    "reduced_output = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=0))(global_avg_output)\n",
    "model_to_save = tf.keras.Model(inputs=[input_tokens], outputs=[reduced_output], name='word2vec_for_abacus')\n",
    "model_to_save.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the embeddings dataframe and naming the first column\n",
    "\n",
    "From the embedding layer that is part of the model we trained, we extract the weights and prepare it for saving it as a CSV. In particular, we will declare the index on the weights dataframe and  name it as “term”. The name of the first column is important, since it defines the key that is used in the JSON output of the prediction api. So continuing with the example provided earlier and the first column of embedding_df being “term”, we can expect the query and response to look like this:\n",
    "```bash\n",
    "> curl --globoff \"http://abacus.ai/api/v0/predict?deploymentToken=foobar&deploymentId=baz&notSent=deadbeef&tokens=[[123,456]]\"\n",
    "{\"success\": true, \"result\": [{\"term\": \"foo\", \"score\": 0.12345678}, {\"term\": \"bar\", \"score\": 1.234567}, ...]}\n",
    "```\n",
    "However, if instead we set the first column's name in the embeddings file to `abacusai`, we would get a different output:\n",
    "```bash\n",
    "> curl --globoff \"http://abacus.ai/api/v0/predict?deploymentToken=foobar&deploymentId=baz&notSent=deadbeef&tokens=[[123,456]]\"\n",
    "{\"success\": true, \"result\": [{\"abacusai\": \"foo\", \"score\": 0.12345678}, {\"abacusai\": \"bar\", \"score\": 1.234567}, ...]}\n",
    "```\n",
    "\n",
    "For this example, we have chosen to stick with `term`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_column_name = 'term'  # This dictates the key used in the output.\n",
    "\n",
    "embedding_weights = model.get_layer(name='embedding').get_weights()[0][1:,:]\n",
    "print(f'Embedding weights: {embedding_weights.shape}')\n",
    "\n",
    "embeddings_df = pd.DataFrame(\n",
    "    embedding_weights,\n",
    "    index=pd.Index(\n",
    "        [encoder.decode([i]).rstrip() for i in range(1, encoder.vocab_size)],\n",
    "        name=item_column_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out all artifacts\n",
    "\n",
    "Now it’s time to generate the required artifacts. For the model, we use the TensorFlow SavedModel format and compress that into a tarball. Then, for the embeddings, we use pandas to write it out as a CSV file. In the end we have 2 artifacts and the folder where the model is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /tmp/word2vec/model\n",
    "saved_model_dir = '/tmp/word2vec/model'\n",
    "model_to_save.save(saved_model_dir)\n",
    "\n",
    "!tar -cvzf /tmp/word2vec/model.tgz -C /tmp/word2vec/model .\n",
    "\n",
    "embeddings_df.to_csv('/tmp/word2vec/embedding.csv')\n",
    "\n",
    "!ls -l /tmp/word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [OPTIONAL] Generate verification data from model and embeddings\n",
    "\n",
    "An optional artifact supported by Abacus.AI is a verification file. This file contains inputs and the corresponding expected outputs for the model. This file can be used to confirm the correctness of the model served by Abacus.AI. For this example, we will be using the cosine distance.\n",
    "An extra optimization made here is the restructuring of the model. Earlier we truncated the model by creating a new model that outputs from the `GlobalAveragePooling1D` layer and added a new lambda to reshape the output into a format expected by Abacus.AI. But for the creation of the verifications file, it can be faster to let the model handle batch inputs and preserve the batch output. So we create a new model, this time only using the output from the `GlobalAveragePooling1D` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model = tf.keras.Model(inputs=[input_tokens], outputs=[model.get_layer('avg_pooling').output], name='word2vec_batch')\n",
    "prediction_model.summary()  # \"new\" model to let TF do batch predictions\n",
    "\n",
    "verification_input = test_batches.unbatch().batch(1).take(10)\n",
    "num_results = 5\n",
    "requests = [{\n",
    "    'input': [[int(x) for x in e[0][0]]],\n",
    "    'num': num_results,\n",
    "    'distance': 'cosine'\n",
    "} for e in list(verification_input.as_numpy_iterator())]\n",
    "\n",
    "prediction_output = prediction_model.predict(verification_input)\n",
    "\n",
    "def norm(m):\n",
    "    return m / np.sqrt(np.sum(m * m, axis=-1, keepdims=1))\n",
    "\n",
    "scores = norm(prediction_output) @ norm(embedding_weights).T\n",
    "\n",
    "examples = prediction_output.shape[0]\n",
    "scored_ix = np.arange(examples).reshape(-1, 1)\n",
    "top_k = scores.argpartition(-num_results)[:,-num_results:]\n",
    "sorted_k = top_k[scored_ix, (scores[scored_ix, top_k]).argsort()]\n",
    "scores_k = scores[scored_ix, sorted_k]\n",
    "\n",
    "# In generating the output shape, note we are re-using the item_column_name variable defined earlier\n",
    "# This is because the key is taken from the name of the first column of the embeddings file.\n",
    "responses = [\n",
    "    {'result': [{item_column_name: encoder.decode([i + 1]).rstrip(), 'score': float(s)}\n",
    "                for i, s in zip(terms, scores)]}\n",
    "    for terms, scores in zip(top_k, scores_k)]\n",
    "\n",
    "# Creating the optional verification file\n",
    "with open('/tmp/word2vec/verification.jsonl', 'wt') as f:\n",
    "    for req, resp in zip(requests, responses):\n",
    "        json.dump({'request': req, 'response': resp}, f)\n",
    "        f.write('\\n')\n",
    "        \n",
    "!ls -l /tmp/word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [RECOMMENDED] Verify saved model\n",
    "\n",
    "Abacus.AI currently does not support defining custom objects. There is a possibility to encounter problems when loading the model. A good check is to load the model that was created earlier from the disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_from_disk = tf.keras.models.load_model(saved_model_dir)\n",
    "model_from_disk.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon loading the model, we can also inspect the structure of the input tensor. It is useful to confirm that the InputLayer was correctly set in the model that was saved. The following is the code similar to that used within Abacus.AI to discover the name of the input tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input Tensors: ', [tensor for tensor in model_from_disk.signatures['serving_default'].structured_input_signature if tensor]) # Cleanup empty inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
