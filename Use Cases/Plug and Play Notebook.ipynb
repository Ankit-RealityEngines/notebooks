{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with Embeddings Artefacts for Plug and Play\n",
    "\n",
    "The plug and play use case provides an opportunity for customers to train their own models and leverage our infrastructure and abstractions to get their models hosted and running on a scalable service with an easy to integrate API. This notebook covers the creation of a model with embeddings, the artefacts that are required by our service, as well as what the resulting API looks like.\n",
    "\n",
    "Below are the imports that we'll be using to create/train the model, as well as generate the artefacts required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Model with Embeddings - Word2Vec\n",
    "\n",
    "To demonstrate the artefacts required as well as provide some recommendations on the overall structure of the model, we will create a model that generates predictions of embedding components and the result is computed as the minimum distance to elements of an embedding dataset with a configurable distance function. The resulting artefacts can be uploaded to the Abacus.AI platform where it can be hosted as a deployment. The artefacts produced are:\n",
    "* tensorflow saved model\n",
    "* embedding dataset\n",
    "* verification samples (optional)\n",
    "\n",
    "So first lets get our data using tensorflow_datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, test_data), info = tfds.load(\n",
    "    'imdb_reviews/subwords8k', \n",
    "    split = (tfds.Split.TRAIN, tfds.Split.TEST), \n",
    "    with_info=True, as_supervised=True)\n",
    "\n",
    "train_batches = train_data.shuffle(1000).padded_batch(10)\n",
    "test_batches = test_data.shuffle(1000).padded_batch(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the model structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = info.features['text'].encoder\n",
    "embedding_dim=16\n",
    "\n",
    "model = K.Sequential([\n",
    "  K.layers.Embedding(encoder.vocab_size, embedding_dim),\n",
    "  K.layers.GlobalAveragePooling1D(),\n",
    "  K.layers.Dense(16, activation='relu'),\n",
    "  K.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we kick off the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_batches,\n",
    "    epochs=10,\n",
    "    validation_data=test_batches, validation_steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting up the model from the embeddings\n",
    "\n",
    "`model` has now been trained, but for use in Abacus.AI, we need to split the embeddings from the model itself. Below we create the `prediction_model` and `embedding_weights`, which will later be saved/formatted for uploading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model = K.models.clone_model(model)\n",
    "prediction_model.pop()\n",
    "prediction_model.pop()\n",
    "prediction_model.summary()\n",
    "\n",
    "print()\n",
    "embedding_weights = model.layers[0].get_weights()[0][1:,:]\n",
    "print(f'Embedding weights: {embedding_weights.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [OPTIONAL] Generate verification data from model and embeddings\n",
    "\n",
    "An optional artefact that is supported is a verifications file. This contains inputs and the corresponding expected outputs for the model. This file can be used to confirm the correctness of the model served by Abacus.AI. For this example, we are using the cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_input = test_batches.unbatch().batch(1).take(10)\n",
    "num_results = 5\n",
    "requests = [{\n",
    "    'input': [[int(x) for x in e[0][0]]],\n",
    "    'num': num_results,\n",
    "    'distance': 'cosine'\n",
    "} for e in list(verification_input.as_numpy_iterator())]\n",
    "\n",
    "prediction_output = prediction_model.predict(verification_input)\n",
    "\n",
    "def norm(m):\n",
    "    return m / np.sqrt(np.sum(m * m, axis=-1, keepdims=1))\n",
    "\n",
    "scores = norm(prediction_output) @ norm(embedding_weights).T\n",
    "\n",
    "examples = prediction_output.shape[0]\n",
    "scored_ix = np.arange(examples).reshape(-1, 1)\n",
    "top_k = scores.argpartition(-num_results)[:,-num_results:]\n",
    "sorted_k = top_k[scored_ix, (scores[scored_ix, top_k]).argsort()]\n",
    "scores_k = scores[scored_ix, sorted_k]\n",
    "\n",
    "responses = [\n",
    "    {'result': [{'term': encoder.decode([i + 1]).rstrip(), 'score': float(s)}\n",
    "                for i, s in zip(terms, scores)]}\n",
    "    for terms, scores in zip(top_k, scores_k)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [IMPORTANT] Update the layers of the created model\n",
    "\n",
    "When the model is hosted, our api accepts multiple types of inputs and needs to be able to determine what gets passed on to the model. To resolve this, we inspect the model to discover its input tensor(s). Below we add a `K.layers.InputLayer` in order to name the input tensor as `tokens`. As a result of this, the prediction api will look for the `tokens` parameter and take whatever the value is, convert it into a tensor and pass it on to the model.\n",
    "\n",
    "With the below definition, lets exampine this example curl request:\n",
    "```bash\n",
    "curl --globoff \"http://abacus.ai/api/v0/predict?deploymentToken=foobar&deploymentId=baz&notSent=deadbeef&tokens=[[123,456]]\"\n",
    "```\n",
    "Of all the query parameters, only `tokens=[[123,456]]` will be converted into a tensor to be passed into the model. The `deploymentToken` and `deploymentId` are required parameters for our API and the `notSent=deadbeef` will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = K.Sequential([\n",
    "    K.layers.InputLayer(input_shape=(None,), name='tokens'),\n",
    "    prediction_model,\n",
    "    K.layers.Lambda(lambda x: tf.reduce_mean(x, axis=0), name='result')\n",
    "])\n",
    "model_to_save.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Keras models, the `InputLayer` is actually hidden. So to confirm that we have successfully named the input tensor we can check with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save.input_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out all Artefacts\n",
    "\n",
    "Now its time to generate the various required artefacts. For the model, we use the TensorFlow SavedModel format and compress that into a tarball. Then, for the embeddings, we use pandas to write it out as a csv. Finally we also create the optional verifications file, which is in a jsonl format. In the end we have 3 artifacts as well as the folder where the SavedModel is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /tmp/word2vec/model\n",
    "saved_model_dir = '/tmp/word2vec/model'\n",
    "model_to_save.save(saved_model_dir)\n",
    "\n",
    "!tar -cvzf /tmp/word2vec/model.tgz -C /tmp/word2vec/model .\n",
    "\n",
    "pd.DataFrame(\n",
    "    embedding_weights,\n",
    "    index=pd.Index(\n",
    "        [encoder.decode([i]).rstrip() for i in range(1, encoder.vocab_size)],\n",
    "        name='term')\n",
    ").to_csv('/tmp/word2vec/embedding.csv')\n",
    "\n",
    "# Creating the optional verification file\n",
    "with open('/tmp/word2vec/verification.jsonl', 'wt') as f:\n",
    "    for req, resp in zip(requests, responses):\n",
    "        json.dump({'request': req, 'response': resp}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "!ls -l /tmp/word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [RECOMMENDED] Verify saved model\n",
    "\n",
    "Abacus.AI currently does not support the definition of custom objects and it is possible there may be other problems encountered when loading the model. A good check is to load the model that we created earlier from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_from_disk = tf.keras.models.load_model(saved_model_dir)\n",
    "model_from_disk.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon loading the model, we can also inspect the structure of the input tensor. This is useful to confirm that the InputLayer was correctly set in the model that was saved. The following is code similar to that used within Abacus.AI to discover the name of the input tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input Tensors: ', [tensor for tensor in model_from_disk.signatures['serving_default'].structured_input_signature if tensor]) # Cleanup empty inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
